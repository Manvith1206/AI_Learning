import pandas as pd
from langchain.embeddings import HuggingFaceEmbeddings
import streamlit as st
from langchain.vectorstores import FAISS
from google import genai

if 'vector_store' not in st.session_state:
    st.session_state.vector_store = None

embedding = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
client = genai.Client(api_key=st.secrets["GEMINI_API_KEY"])

def converCsvToNLData():
    # Load your CSV
    df = pd.read_csv("RAG/IPL_Ball_by_Ball_2008_2022.csv")

    # Fallback replacements
    def safe(val):
        return val if pd.notna(val) else "NA"

    # Convert each row to a descriptive sentence
    descriptions = []

    for _, row in df.iterrows():
        match_id = row['ID']
        innings = row['innings']
        over = f"{row['overs']}.{row['ballnumber']}"
        batter = row['batter']
        bowler = row['bowler']
        non_striker = row['non-striker']
        extra_type = safe(row['extra_type'])
        batsman_run = row['batsman_run']
        extras_run = row['extras_run']
        total_run = row['total_run']
        is_wicket = "Yes" if row['isWicketDelivery'] else "No"
        player_out = safe(row['player_out'])
        kind = safe(row['kind'])
        fielders = safe(row['fielders_involved'])
        team = row['BattingTeam']

        # Describe the delivery
        line = f"In match {match_id}, innings {innings}, over {over}, {batter} faced {bowler}. "

        if extra_type != "NA":
            line += f"The delivery resulted in an extra ({extra_type}) for {extras_run} run(s). "
        elif batsman_run > 0:
            line += f"The batter scored {batsman_run} run(s). "
        else:
            line += "No run was scored. "

        if is_wicket == "Yes":
            line += f"{player_out} was dismissed ({kind}). Fielders involved: {fielders}. "

        line += f"Total runs from this ball: {total_run}. Batting team: {team}."

        descriptions.append(line)

    # Save to a text file
    with open("descriptive_match_data.txt", "w") as f:
        for line in descriptions:
            f.write(line + "\n")

def main():
    # converCsvToNLData()

    with open("descriptive_match_data.txt", "r") as f:
        lines = [line.strip() for line in f.readlines() if line.strip()]

    if lines:
        st.session_state.vector_store = FAISS.from_documents(lines, embedding)


    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Display chat history
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            
    if prompt := st.chat_input("Ask a question about your documents"):
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # Display user message
        with st.chat_message("user"):
            st.markdown(prompt)
            
        # Generate and display assistant response
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                # Debug: Display vector store status
                if st.session_state.vector_store is not None:
                    # Create RAG pipeline with proper configuration
                    similar_docs_with_scores = st.session_state.vector_store.similarity_search_with_relevance_scores(prompt, k=2)

                    context_parts = []
                    for doc, score in similar_docs_with_scores:
                        context_parts.append(f"Document ID: {doc.id} (Similarity: {score:.4f})\n{doc.page_content}")
                    
                    context = "\n\n".join(context_parts)

                    query = f"""
                        Based on the following context, please answer the question.
                        
                        Context:
                        {context}
                        
                        Question: {prompt}
                        
                        Answer:
                        """
                    response = client.models.generate_content(model="gemini-2.0-flash", contents=query)

                    # Display the response
                    st.markdown(response.text)
                    
                    # Add assistant response to chat history
                    st.session_state.messages.append({"role": "assistant", "content": response.text})
                    context = context + "\n\n" + response.text
                else:
                    st.error("Please upload and process documents first.")